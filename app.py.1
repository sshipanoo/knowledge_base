import os
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.llms import LlamaCpp

app = FastAPI()

# 1. 初始化LLM
model_path = "/home/ubuntu/deploy/mistral-q4_0.gguf"  # 替换为您的模型路径
llm = LlamaCpp(
    model_path=model_path,
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=0,  # 设置为0以使用CPU
    temperature=0.7,
    max_tokens=512,
    verbose=True
)

# 2. 初始化VectorStore
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-base-zh-v1.5",
    model_kwargs={"device": "cpu"}  # 使用CPU
)
documents = [
    "LangChain 是一个用于构建大语言模型应用的框架。",
    "FAISS 是一个高效的向量相似度搜索库。",
    "ChatGLM3-6B 是一个开源的中英双语对话模型。"
]
vectorstore = FAISS.from_texts(documents, embeddings)

#vectorstore = FAISS.from_texts([], embeddings)  # 初始化为空

# 3. 定义Prompt模板
prompt_template = """基于以下上下文和你的知识，用中文回答：
上下文：{context}
问题：{question}
答案："""

QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
    return_source_documents=True
)

# 4. 文件上传接口
@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        # 保存上传的文件
        upload_dir = './uploads'
        os.makedirs(upload_dir, exist_ok=True)
        file_path = os.path.join(upload_dir, file.filename)
        with open(file_path, 'wb') as f:
            content = await file.read()
            f.write(content)

        # 加载文件内容
        loader = DirectoryLoader(upload_dir, glob="**/*.txt")
        documents = loader.load()

        # 分割文档
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, chunk_overlap=50, separators=["\n\n", "\n", "。", "！", "？"]
        )
        splits = text_splitter.split_documents(documents)

        # 更新向量数据库
        vectorstore.add_documents(splits)

        return {"filename": file.filename, "message": "文件上传并处理成功"}
    except Exception as e:
        return JSONResponse(status_code=500, content={"message": f"文件处理失败: {str(e)}"})

# 5. 问答接口
@app.get("/ask/{query}")
async def ask(query: str):
    try:
        result = qa_chain({"query": query})
        answer = result["result"]
        sources = [doc.page_content for doc in result["source_documents"]]
        return {"answer": answer, "sources": sources}
    except Exception as e:
        return JSONResponse(status_code=500, content={"message": f"问答处理失败: {str(e)}"})

